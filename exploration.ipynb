{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser\n",
    "\n",
    "ds = load_dataset(\"alpindale/two-million-bluesky-posts\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First post:\n",
      " {'text': \"This is really interesting polling data about national public attitudes re: California.  It's from the LA Times, in January.  I wonder if this will change substantially in the next two years?  5233025.fs1.hubspotusercontent-na1.net/hubfs/523302...\", 'created_at': '2024-11-27T07:53:47.202Z', 'author': 'did:plc:5ug6fzthlj6yyvftj3alekpj', 'uri': 'at://did:plc:5ug6fzthlj6yyvftj3alekpj/app.bsky.feed.post/3lbw33zxvik24', 'has_images': False, 'reply_to': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"First post:\\n\", ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 16866 posts in the hour 07:00‚Äì08:00 (no language predictions available).\n",
      "\n",
      "{'text': \"This is really interesting polling data about national public attitudes re: California.  It's from the LA Times, in January.  I wonder if this will change substantially in the next two years?  5233025.fs1.hubspotusercontent-na1.net/hubfs/523302...\", 'created_at': '2024-11-27T07:53:47.202Z', 'author': 'did:plc:5ug6fzthlj6yyvftj3alekpj', 'uri': 'at://did:plc:5ug6fzthlj6yyvftj3alekpj/app.bsky.feed.post/3lbw33zxvik24', 'has_images': False, 'reply_to': None}\n",
      "{'text': 'Niet been, iets.', 'created_at': '2024-11-27T07:53:46.656Z', 'author': 'did:plc:xfvxxrfblwuc3kdthl344vc5', 'uri': 'at://did:plc:xfvxxrfblwuc3kdthl344vc5/app.bsky.feed.post/3lbw33zh7cs2l', 'has_images': False, 'reply_to': 'at://did:plc:fwjmfthdu5wppad75pcgpalj/app.bsky.feed.post/3lbvzv3tuc226'}\n",
      "{'text': '„Çà„ÅèËÄÉ„Åà„Åü„Çâchr„Ååsb„Åï„Çì„ÅÆËÖ∞„Çí‰∏°ËÑö„ÅßÊåü„Çì„Åß„Å©„ÅÜ„Åì„ÅÜ„Å®„Åã„Éù„Çπ„Éà„Åó„Å¶„Åü„Å™\\n„ÅÑ„Åæ„Åï„Çâ„Å†„Å£„Åü‚Ä¶‚Ä¶', 'created_at': '2024-11-27T07:53:46.554Z', 'author': 'did:plc:33m5bavcbydbgxqv7lzukclo', 'uri': 'at://did:plc:33m5bavcbydbgxqv7lzukclo/app.bsky.feed.post/3lbw33ze3pc2x', 'has_images': False, 'reply_to': None}\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime(2024, 11, 27, 7, 0, 0, tzinfo=timezone.utc)\n",
    "end_time   = datetime(2024, 11, 27, 8, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "def in_target_hour(example):\n",
    "    # Replace trailing 'Z' with '+00:00'\n",
    "    aware_str = example[\"created_at\"].replace(\"Z\", \"+00:00\")\n",
    "    dt = parser.isoparse(aware_str)  # <-- dateutil's tolerant parser\n",
    "    return start_time <= dt < end_time\n",
    "\n",
    "subset_time = ds.filter(in_target_hour)\n",
    "# If you have predicted_language, filter to English:\n",
    "def is_english(example):\n",
    "    return example.get('predicted_language', '') == 'en'\n",
    "\n",
    "if 'predicted_language' in ds.column_names:\n",
    "    subset_en = subset_time.filter(is_english)\n",
    "    print(f\"\\nFound {len(subset_en)} English posts in the hour 07:00‚Äì08:00.\\n\")\n",
    "    for i in range(min(3, len(subset_en))):\n",
    "        print(subset_en[i])\n",
    "else:\n",
    "    print(f\"\\nFound {len(subset_time)} posts in the hour 07:00‚Äì08:00 (no language predictions available).\\n\")\n",
    "    for i in range(min(3, len(subset_time))):\n",
    "        print(subset_time[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "\n",
    "from huggingface_hub import (\n",
    "    notebook_login,\n",
    "    create_inference_endpoint,\n",
    "    list_inference_endpoints,\n",
    "    whoami\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timezone\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5f4bb25f0241c6910796244555bfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently logged in as: amuralit\n"
     ]
    }
   ],
   "source": [
    "notebook_login()\n",
    "\n",
    "who = whoami()\n",
    "print(\"Currently logged in as:\", who[\"name\"])\n",
    "\n",
    "# If you belong to an org, and want to charge it instead of your personal account,\n",
    "# you can prompt for it or just set it manually here:\n",
    "# organization = getpass(\"What is your Hugging Face ü§ó organization (with payment method)? \")\n",
    "# namespace = organization or who[\"name\"]\n",
    "namespace = who[\"name\"]  # or an organization name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new endpoint: my-bluesky-embedding-endpoint\n",
      "\n",
      "Waiting for endpoint to be running...\n"
     ]
    },
    {
     "ename": "InferenceEndpointError",
     "evalue": "Inference Endpoint my-bluesky-embedding-endpoint failed to deploy. Please check the logs for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceEndpointError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Wait until the endpoint is running\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWaiting for endpoint to be running...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpoint is running!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/_inference_endpoints.py:211\u001b[0m, in \u001b[0;36mInferenceEndpoint.wait\u001b[0;34m(self, timeout, refresh_every)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m InferenceEndpointStatus\u001b[38;5;241m.\u001b[39mFAILED:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceEndpointError(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference Endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed to deploy. Please check the logs for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m>\u001b[39m timeout:\n",
      "\u001b[0;31mInferenceEndpointError\u001b[0m: Inference Endpoint my-bluesky-embedding-endpoint failed to deploy. Please check the logs for more information."
     ]
    }
   ],
   "source": [
    "##\n",
    "# 2. Create/Load an Inference Endpoint\n",
    "##\n",
    "ENDPOINT_NAME = \"my-bluesky-embedding-endpoint\"\n",
    "\n",
    "# These were used in Derek's tutorial:\n",
    "VENDOR = \"aws\"\n",
    "REGION = \"us-east-1\"\n",
    "INSTANCE_SIZE = \"x1\"         # Single GPU\n",
    "INSTANCE_TYPE = \"nvidia-a100\"\n",
    "\n",
    "MAX_WORKERS = 5  # How many async workers (and a factor in how many tokens we can handle at once)\n",
    "\n",
    "# Create or load the Inference Endpoint\n",
    "try:\n",
    "    endpoint = create_inference_endpoint(\n",
    "        name=ENDPOINT_NAME,\n",
    "        repository=\"nvidia/NV-Embed-v2\",\n",
    "        task=\"sentence-embeddings\",\n",
    "        framework=\"pytorch\",\n",
    "        accelerator=\"gpu\",\n",
    "        instance_size=INSTANCE_SIZE,\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        region=REGION,\n",
    "        max_position_embeddings=32768,\n",
    "        vendor=VENDOR,\n",
    "        namespace=namespace,\n",
    "        custom_image={\n",
    "            \"health_route\": \"/health\",\n",
    "            \"env\": {\n",
    "                \"MAX_BATCH_TOKENS\": str(MAX_WORKERS * 2048),\n",
    "                \"MAX_CONCURRENT_REQUESTS\": \"512\",\n",
    "                \"MODEL_ID\": \"/repository\",\n",
    "            },\n",
    "            \"url\": \"ghcr.io/huggingface/text-embeddings-inference:0.5.0\",\n",
    "        },\n",
    "        type=\"protected\",\n",
    "    )\n",
    "    print(f\"Created new endpoint: {ENDPOINT_NAME}\")\n",
    "except Exception as e:\n",
    "    # If the endpoint already exists, we load it from the list\n",
    "    print(\"Endpoint already exists or could not be created, loading existing endpoint...\")\n",
    "    endpoint = [ie for ie in list_inference_endpoints(namespace=namespace) if ie.name == ENDPOINT_NAME][0]\n",
    "\n",
    "# Wait until the endpoint is running\n",
    "print(\"\\nWaiting for endpoint to be running...\")\n",
    "endpoint.wait()\n",
    "print(\"Endpoint is running!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding this text:\n",
      " This is really interesting polling data about national public attitudes re: California.  It's from the LA Times, in January.  I wonder if this will change substantially in the next two years?  5233025.fs1.hubspotusercontent-na1.net/hubfs/523302...\n",
      "\n",
      "Embedded vector shape: (1, 768)\n",
      "First 10 values of the embedding:\n",
      " [-0.04396549 -0.07015622  0.07088956  0.03038124 -0.02131925 -0.02956059\n",
      "  0.01464934 -0.04396549  0.06275297  0.02435737]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# 4. Embed one sample text from subset_en\n",
    "##\n",
    "example_to_embed = subset_time[0][\"text\"]\n",
    "print(\"Embedding this text:\\n\", example_to_embed)\n",
    "\n",
    "# Send the text to your Inference Endpoint\n",
    "response_bytes = endpoint.client.post(\n",
    "    json={\n",
    "        \"inputs\": example_to_embed,\n",
    "        \"truncate\": True  # If text is too long, we truncate it\n",
    "    },\n",
    "    task=\"feature-extraction\",\n",
    ")\n",
    "# Convert response from bytes -> JSON -> NumPy array\n",
    "embedding_array = np.array(json.loads(response_bytes.decode()))\n",
    "\n",
    "print(\"\\nEmbedded vector shape:\", embedding_array.shape)\n",
    "print(\"First 10 values of the embedding:\\n\", embedding_array[0][:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
